{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBcRmQLT6GXT",
        "outputId": "344a302f-9edb-4de8-92c1-7e0f296e9e0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer.word_index: ${'the': 1, 'was': 2, 'i': 3, 'a': 4, 'it': 5, \"i'm\": 6, 'at': 7, 'to': 8, 'had': 9, 'my': 10, 'about': 11, 'food': 12, 'good': 13, 'new': 14, 'feeling': 15, 'of': 16, 'with': 17, 'movie': 18, 'restaurant': 19, 'in': 20, 'concert': 21, 'party': 22, \"couldn't\": 23, 'service': 24, 'for': 25, 'just': 26, 'last': 27, 'night': 28, 'performance': 29, 'experience': 30, 'lot': 31, 'things': 32, 'up': 33, 'towards': 34, 'me': 35, 'upcoming': 36, 'not': 37, 'sure': 38, 'get': 39, 'but': 40, \"i've\": 41, 'fantastic': 42, 'every': 43, 'terrible': 44, 'weather': 45, 'is': 46, 'pleasant': 47, 'perfect': 48, 'park': 49, 'disappointed': 50, 'put': 51, 'on': 52, 'great': 53, 'neutral': 54, 'nothing': 55, 'or': 56, 'bad': 57, 'software': 58, 'update': 59, 'optimistic': 60, 'future': 61, 'seem': 62, 'be': 63, 'looking': 64, 'traffic': 65, 'delicious': 66, 'indifferent': 67, 'customer': 68, 'helpful': 69, 'they': 70, 'issue': 71, 'day': 72, 'go': 73, 'book': 74, 'feel': 75, 'if': 76, 'what': 77, 'hotel': 78, 'room': 79, 'and': 80, 'boring': 81, 'excited': 82, 'opportunity': 83, 'travel': 84, \"it's\": 85, 'something': 86, 'do': 87, 'exhibit': 88, 'museum': 89, 'learned': 90, 'apathetic': 91, 'tickets': 92, 'were': 93, 'expensive': 94, 'worth': 95, 'time': 96, 'constant': 97, 'has': 98, 'from': 99, \"didn't\": 100, 'live': 101, 'loved': 102, 'minute': 103, 'waited': 104, 'over': 105, 'an': 106, 'hour': 107, 'today': 108, 'quite': 109, 'walk': 110, 'quality': 111, 'product': 112, 'broke': 113, 'after': 114, 'one': 115, 'use': 116, 'amazing': 117, 'band': 118, 'store': 119, 'particularly': 120, 'happened': 121, 'causing': 122, 'problems': 123, 'computer': 124, 'keeps': 125, 'crashing': 126, 'this': 127, 'morning': 128, 'stuck': 129, 'gridlock': 130, 'hours': 131, 'stop': 132, 'eating': 133, 'policy': 134, \"doesn't\": 135, 'really': 136, 'affect': 137, 'either': 138, 'way': 139, 'representative': 140, 'very': 141, 'resolved': 142, 'quickly': 143, 'horrible': 144, 'work': 145, 'everything': 146, 'that': 147, 'could': 148, 'wrong': 149, 'did': 150, 'finished': 151, 'reading': 152, 'captivating': 153, 'down': 154, 'ambivalent': 155, 'changes': 156, \"they'll\": 157, 'positive': 158, 'negative': 159, 'vacation': 160, 'relaxing': 161, 'needed': 162, 'recharge': 163, 'dirty': 164, 'noisy': 165, \"night's\": 166, 'sleep': 167, 'mixed': 168, 'slow': 169, 'watched': 170, 'kept': 171, 'checking': 172, 'watch': 173, 'always': 174, 'wanted': 175, 'disaster': 176, 'went': 177, 'according': 178, 'plan': 179, 'fascinating': 180, 'election': 181, 'none': 182, 'candidates': 183, 'appealing': 184, 'show': 185, 'penny': 186, 'conversation': 187, 'stranger': 188, 'bus': 189, 'made': 190, 'wonderful': 191, 'beach': 192, 'support': 193, 'assisted': 194, 'promptly': 195, 'frustrated': 196, 'delays': 197, 'public': 198, 'transportation': 199, 'predictable': 200, 'plot': 201, 'town': 202, 'highly': 203, 'recommend': 204, 'event': 205, 'want': 206, 'read': 207, 'into': 208, 'been': 209, 'dreaming': 210, 'presentation': 211, 'informative': 212, 'unbearable': 213, 'took': 214, 'twice': 215, 'as': 216, 'long': 217, 'home': 218, 'blast': 219, 'so': 220, 'much': 221, 'fun': 222, 'dancing': 223, 'friends': 224, 'believe': 225, 'are': 226, 'coming': 227, 'disappointing': 228, 'hype': 229, 'current': 230, 'political': 231, 'situation': 232, 'seems': 233, 'hopeless': 234, 'underwhelming': 235, 'expected': 236, 'more': 237, 'theme': 238, 'neither': 239, 'nor': 240, 'spacious': 241, 'clean': 242, 'felt': 243, 'comfortable': 244, 'during': 245, 'stay': 246, 'annoyed': 247, 'by': 248, 'noise': 249, 'construction': 250, 'site': 251, 'next': 252, 'door': 253, 'flop': 254, 'reviews': 255, 'conflicted': 256, 'decision': 257, 'mediocre': 258, 'better': 259, 'music': 260, 'improved': 261, \"haven't\": 262, 'experienced': 263, 'any': 264, 'crashes': 265, 'pessimistic': 266, 'outcome': 267, \"aren't\": 268}\n",
            "sequences: $[[1, 18, 2, 42, 3, 102, 43, 103, 16, 5], [1, 24, 7, 1, 19, 2, 44, 3, 104, 105, 106, 107, 25, 10, 12], [1, 45, 108, 46, 109, 47, 48, 25, 4, 110, 20, 1, 49], [3, 2, 50, 17, 1, 111, 16, 1, 112, 5, 113, 114, 26, 115, 116], [1, 21, 27, 28, 2, 117, 1, 118, 51, 52, 4, 53, 29], [3, 9, 4, 54, 30, 7, 1, 119, 55, 120, 13, 56, 57, 121], [1, 14, 58, 59, 46, 122, 4, 31, 16, 123, 10, 124, 125, 126], [6, 15, 60, 11, 1, 61, 32, 62, 8, 63, 64, 33], [1, 65, 127, 128, 2, 44, 3, 2, 129, 20, 130, 25, 131], [1, 12, 7, 1, 22, 2, 66, 3, 23, 132, 133], [6, 67, 34, 1, 14, 134, 5, 135, 136, 137, 35, 138, 139], [1, 68, 24, 140, 2, 141, 69, 70, 142, 10, 71, 143], [3, 9, 4, 144, 72, 7, 145, 146, 147, 148, 73, 149, 150], [1, 74, 3, 26, 151, 152, 2, 153, 3, 23, 51, 5, 154], [3, 75, 155, 11, 1, 36, 156, 6, 37, 38, 76, 157, 63, 158, 56, 159], [1, 160, 2, 161, 26, 77, 3, 162, 8, 163], [1, 78, 79, 2, 164, 80, 165, 3, 23, 39, 4, 13, 166, 167], [3, 9, 4, 168, 30, 7, 1, 19, 1, 12, 2, 13, 40, 1, 24, 2, 169], [1, 18, 3, 170, 27, 28, 2, 81, 3, 171, 172, 10, 173], [6, 82, 11, 1, 83, 8, 84, 85, 86, 41, 174, 175, 8, 87], [1, 22, 2, 4, 176, 55, 177, 178, 8, 179], [1, 14, 88, 7, 1, 89, 2, 180, 3, 90, 4, 31], [3, 75, 91, 34, 1, 36, 181, 182, 16, 1, 183, 62, 184], [1, 21, 92, 93, 94, 40, 1, 185, 2, 95, 43, 186], [3, 9, 4, 47, 187, 17, 4, 188, 52, 1, 189, 5, 190, 10, 72], [3, 9, 4, 191, 96, 7, 1, 192, 1, 45, 2, 48], [1, 68, 193, 2, 69, 70, 194, 35, 17, 10, 71, 195], [6, 196, 17, 1, 97, 197, 20, 198, 199], [1, 18, 9, 4, 200, 201, 3, 2, 50], [1, 14, 19, 20, 202, 98, 66, 12, 3, 203, 204, 5], [6, 15, 67, 34, 1, 36, 205, 6, 37, 38, 76, 3, 206, 8, 73], [1, 74, 3, 207, 27, 28, 2, 81, 3, 23, 39, 208, 5], [6, 82, 11, 1, 83, 8, 84, 85, 86, 41, 209, 210, 16], [1, 211, 2, 212, 3, 90, 4, 31, 99, 5], [1, 65, 2, 213, 5, 214, 35, 215, 216, 217, 8, 39, 218], [1, 22, 2, 4, 219, 3, 9, 220, 221, 222, 223, 17, 10, 224], [6, 15, 60, 11, 1, 61, 3, 225, 13, 32, 226, 227], [1, 12, 7, 1, 14, 19, 2, 228, 5, 100, 101, 33, 8, 1, 229], [1, 21, 92, 93, 94, 40, 1, 29, 2, 95, 5], [6, 15, 91, 11, 1, 230, 231, 232, 5, 233, 234], [1, 89, 88, 2, 235, 3, 236, 237], [3, 9, 4, 54, 30, 7, 1, 238, 49, 5, 2, 239, 13, 240, 57], [1, 78, 79, 2, 241, 80, 242, 3, 243, 244, 245, 10, 246], [6, 247, 248, 1, 97, 249, 99, 1, 250, 251, 252, 253], [1, 18, 2, 4, 254, 5, 100, 101, 33, 8, 1, 255], [6, 15, 256, 11, 1, 257, 6, 37, 38, 77, 8, 87], [1, 12, 7, 1, 22, 2, 258, 41, 9, 259], [3, 9, 4, 53, 96, 7, 1, 21, 1, 260, 2, 42], [1, 14, 58, 59, 98, 261, 29, 3, 262, 263, 264, 265], [6, 15, 266, 11, 1, 267, 32, 268, 64, 13]]\n",
            "padded_sequences: $[[  0   0   0   0   0   0   0   1  18   2  42   3 102  43 103  16   5]\n",
            " [  0   0   1  24   7   1  19   2  44   3 104 105 106 107  25  10  12]\n",
            " [  0   0   0   0   1  45 108  46 109  47  48  25   4 110  20   1  49]\n",
            " [  0   0   3   2  50  17   1 111  16   1 112   5 113 114  26 115 116]\n",
            " [  0   0   0   0   1  21  27  28   2 117   1 118  51  52   4  53  29]\n",
            " [  0   0   0   3   9   4  54  30   7   1 119  55 120  13  56  57 121]\n",
            " [  0   0   0   1  14  58  59  46 122   4  31  16 123  10 124 125 126]\n",
            " [  0   0   0   0   0   6  15  60  11   1  61  32  62   8  63  64  33]\n",
            " [  0   0   0   0   1  65 127 128   2  44   3   2 129  20 130  25 131]\n",
            " [  0   0   0   0   0   0   1  12   7   1  22   2  66   3  23 132 133]\n",
            " [  0   0   0   0   6  67  34   1  14 134   5 135 136 137  35 138 139]\n",
            " [  0   0   0   0   0   1  68  24 140   2 141  69  70 142  10  71 143]\n",
            " [  0   0   0   0   3   9   4 144  72   7 145 146 147 148  73 149 150]\n",
            " [  0   0   0   0   1  74   3  26 151 152   2 153   3  23  51   5 154]\n",
            " [  0   3  75 155  11   1  36 156   6  37  38  76 157  63 158  56 159]\n",
            " [  0   0   0   0   0   0   0   1 160   2 161  26  77   3 162   8 163]\n",
            " [  0   0   0   1  78  79   2 164  80 165   3  23  39   4  13 166 167]\n",
            " [  3   9   4 168  30   7   1  19   1  12   2  13  40   1  24   2 169]\n",
            " [  0   0   0   0   1  18   3 170  27  28   2  81   3 171 172  10 173]\n",
            " [  0   0   0   6  82  11   1  83   8  84  85  86  41 174 175   8  87]\n",
            " [  0   0   0   0   0   0   0   1  22   2   4 176  55 177 178   8 179]\n",
            " [  0   0   0   0   0   1  14  88   7   1  89   2 180   3  90   4  31]\n",
            " [  0   0   0   0   3  75  91  34   1  36 181 182  16   1 183  62 184]\n",
            " [  0   0   0   0   0   1  21  92  93  94  40   1 185   2  95  43 186]\n",
            " [  0   0   3   9   4  47 187  17   4 188  52   1 189   5 190  10  72]\n",
            " [  0   0   0   0   0   3   9   4 191  96   7   1 192   1  45   2  48]\n",
            " [  0   0   0   0   0   1  68 193   2  69  70 194  35  17  10  71 195]\n",
            " [  0   0   0   0   0   0   0   0   6 196  17   1  97 197  20 198 199]\n",
            " [  0   0   0   0   0   0   0   0   1  18   9   4 200 201   3   2  50]\n",
            " [  0   0   0   0   0   1  14  19  20 202  98  66  12   3 203 204   5]\n",
            " [  0   0   6  15  67  34   1  36 205   6  37  38  76   3 206   8  73]\n",
            " [  0   0   0   0   1  74   3 207  27  28   2  81   3  23  39 208   5]\n",
            " [  0   0   0   0   6  82  11   1  83   8  84  85  86  41 209 210  16]\n",
            " [  0   0   0   0   0   0   0   1 211   2 212   3  90   4  31  99   5]\n",
            " [  0   0   0   0   1  65   2 213   5 214  35 215 216 217   8  39 218]\n",
            " [  0   0   0   1  22   2   4 219   3   9 220 221 222 223  17  10 224]\n",
            " [  0   0   0   0   0   6  15  60  11   1  61   3 225  13  32 226 227]\n",
            " [  0   0   1  12   7   1  14  19   2 228   5 100 101  33   8   1 229]\n",
            " [  0   0   0   0   0   0   1  21  92  93  94  40   1  29   2  95   5]\n",
            " [  0   0   0   0   0   0   6  15  91  11   1 230 231 232   5 233 234]\n",
            " [  0   0   0   0   0   0   0   0   0   1  89  88   2 235   3 236 237]\n",
            " [  0   0   3   9   4  54  30   7   1 238  49   5   2 239  13 240  57]\n",
            " [  0   0   0   0   1  78  79   2 241  80 242   3 243 244 245  10 246]\n",
            " [  0   0   0   0   0   6 247 248   1  97 249  99   1 250 251 252 253]\n",
            " [  0   0   0   0   0   1  18   2   4 254   5 100 101  33   8   1 255]\n",
            " [  0   0   0   0   0   6  15 256  11   1 257   6  37  38  77   8  87]\n",
            " [  0   0   0   0   0   0   0   1  12   7   1  22   2 258  41   9 259]\n",
            " [  0   0   0   0   0   3   9   4  53  96   7   1  21   1 260   2  42]\n",
            " [  0   0   0   0   0   1  14  58  59  98 261  29   3 262 263 264 265]\n",
            " [  0   0   0   0   0   0   0   6  15 266  11   1 267  32 268  64  13]]\n",
            "labels: #[[0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]]\n",
            "Epoch 1/30\n",
            "2/2 [==============================] - 3s 609ms/step - loss: 1.0979 - accuracy: 0.2500 - val_loss: 1.0953 - val_accuracy: 0.4000\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 1.0860 - accuracy: 0.5000 - val_loss: 1.0934 - val_accuracy: 0.4000\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.0754 - accuracy: 0.4750 - val_loss: 1.0913 - val_accuracy: 0.4000\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 1.0610 - accuracy: 0.4750 - val_loss: 1.0898 - val_accuracy: 0.4000\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 1.0423 - accuracy: 0.4750 - val_loss: 1.0899 - val_accuracy: 0.4000\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 1.0237 - accuracy: 0.4750 - val_loss: 1.0952 - val_accuracy: 0.4000\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.9950 - accuracy: 0.4750 - val_loss: 1.1163 - val_accuracy: 0.4000\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.9603 - accuracy: 0.4750 - val_loss: 1.1790 - val_accuracy: 0.4000\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.9537 - accuracy: 0.4750 - val_loss: 1.2437 - val_accuracy: 0.4000\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.9352 - accuracy: 0.4750 - val_loss: 1.1986 - val_accuracy: 0.4000\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.9026 - accuracy: 0.4750 - val_loss: 1.1670 - val_accuracy: 0.4000\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.8648 - accuracy: 0.4750 - val_loss: 1.1721 - val_accuracy: 0.4000\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.8322 - accuracy: 0.6000 - val_loss: 1.1920 - val_accuracy: 0.4000\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.7877 - accuracy: 0.7000 - val_loss: 1.2489 - val_accuracy: 0.4000\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.7318 - accuracy: 0.7000 - val_loss: 1.3968 - val_accuracy: 0.4000\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.6893 - accuracy: 0.7000 - val_loss: 1.5168 - val_accuracy: 0.2000\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.6377 - accuracy: 0.7250 - val_loss: 1.4195 - val_accuracy: 0.2000\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.5665 - accuracy: 0.7750 - val_loss: 1.3526 - val_accuracy: 0.2000\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.5399 - accuracy: 0.8000 - val_loss: 1.4605 - val_accuracy: 0.2000\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.4822 - accuracy: 0.8000 - val_loss: 1.6292 - val_accuracy: 0.2000\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.4221 - accuracy: 0.8000 - val_loss: 1.5386 - val_accuracy: 0.2000\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.3885 - accuracy: 0.8000 - val_loss: 1.4720 - val_accuracy: 0.2000\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.3629 - accuracy: 0.8000 - val_loss: 1.5155 - val_accuracy: 0.2000\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.3337 - accuracy: 0.8500 - val_loss: 1.6263 - val_accuracy: 0.1000\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.3043 - accuracy: 0.8500 - val_loss: 1.7705 - val_accuracy: 0.1000\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.2749 - accuracy: 0.9000 - val_loss: 1.8874 - val_accuracy: 0.1000\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.2478 - accuracy: 0.9500 - val_loss: 2.0337 - val_accuracy: 0.1000\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.2218 - accuracy: 0.9500 - val_loss: 2.1601 - val_accuracy: 0.1000\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.1980 - accuracy: 0.9500 - val_loss: 2.1907 - val_accuracy: 0.1000\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.1759 - accuracy: 0.9500 - val_loss: 2.2497 - val_accuracy: 0.1000\n",
            "1/1 [==============================] - 1s 673ms/step\n",
            "Predicted tone: Positive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "texts = [\n",
        "    \"The movie was fantastic, I loved every minute of it.\",\n",
        "    \"The service at the restaurant was terrible, I waited over an hour for my food.\",\n",
        "    \"The weather today is quite pleasant, perfect for a walk in the park.\",\n",
        "    \"I was disappointed with the quality of the product, it broke after just one use.\",\n",
        "    \"The concert last night was amazing, the band put on a great performance.\",\n",
        "    \"I had a neutral experience at the store, nothing particularly good or bad happened.\",\n",
        "    \"The new software update is causing a lot of problems, my computer keeps crashing.\",\n",
        "    \"I'm feeling optimistic about the future, things seem to be looking up.\",\n",
        "    \"The traffic this morning was terrible, I was stuck in gridlock for hours.\",\n",
        "    \"The food at the party was delicious, I couldn't stop eating.\",\n",
        "    \"I'm indifferent towards the new policy, it doesn't really affect me either way.\",\n",
        "    \"The customer service representative was very helpful, they resolved my issue quickly.\",\n",
        "    \"I had a horrible day at work, everything that could go wrong did.\",\n",
        "    \"The book I just finished reading was captivating, I couldn't put it down.\",\n",
        "    \"I feel ambivalent about the upcoming changes, I'm not sure if they'll be positive or negative.\",\n",
        "    \"The vacation was relaxing, just what I needed to recharge.\",\n",
        "    \"The hotel room was dirty and noisy, I couldn't get a good night's sleep.\",\n",
        "    \"I had a mixed experience at the restaurant, the food was good but the service was slow.\",\n",
        "    \"The movie I watched last night was boring, I kept checking my watch.\",\n",
        "    \"I'm excited about the opportunity to travel, it's something I've always wanted to do.\",\n",
        "    \"The party was a disaster, nothing went according to plan.\",\n",
        "    \"The new exhibit at the museum was fascinating, I learned a lot.\",\n",
        "    \"I feel apathetic towards the upcoming election, none of the candidates seem appealing.\",\n",
        "    \"The concert tickets were expensive, but the show was worth every penny.\",\n",
        "    \"I had a pleasant conversation with a stranger on the bus, it made my day.\",\n",
        "    \"I had a wonderful time at the beach, the weather was perfect.\",\n",
        "    \"The customer support was helpful, they assisted me with my issue promptly.\",\n",
        "    \"I'm frustrated with the constant delays in public transportation.\",\n",
        "    \"The movie had a predictable plot, I was disappointed.\",\n",
        "    \"The new restaurant in town has delicious food, I highly recommend it.\",\n",
        "    \"I'm feeling indifferent towards the upcoming event, I'm not sure if I want to go.\",\n",
        "    \"The book I read last night was boring, I couldn't get into it.\",\n",
        "    \"I'm excited about the opportunity to travel, it's something I've been dreaming of.\",\n",
        "    \"The presentation was informative, I learned a lot from it.\",\n",
        "    \"The traffic was unbearable, it took me twice as long to get home.\",\n",
        "    \"The party was a blast, I had so much fun dancing with my friends.\",\n",
        "    \"I'm feeling optimistic about the future, I believe good things are coming.\",\n",
        "    \"The food at the new restaurant was disappointing, it didn't live up to the hype.\",\n",
        "    \"The concert tickets were expensive, but the performance was worth it.\",\n",
        "    \"I'm feeling apathetic about the current political situation, it seems hopeless.\",\n",
        "    \"The museum exhibit was underwhelming, I expected more.\",\n",
        "    \"I had a neutral experience at the theme park, it was neither good nor bad.\",\n",
        "    \"The hotel room was spacious and clean, I felt comfortable during my stay.\",\n",
        "    \"I'm annoyed by the constant noise from the construction site next door.\",\n",
        "    \"The movie was a flop, it didn't live up to the reviews.\",\n",
        "    \"I'm feeling conflicted about the decision, I'm not sure what to do.\",\n",
        "    \"The food at the party was mediocre, I've had better.\",\n",
        "    \"I had a great time at the concert, the music was fantastic.\",\n",
        "    \"The new software update has improved performance, I haven't experienced any crashes.\",\n",
        "    \"I'm feeling pessimistic about the outcome, things aren't looking good.\"\n",
        "]\n",
        "\n",
        "labels = [\n",
        "    1, 0, 1, 0, 1, 2, 0, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 1, 0,\n",
        "    1, 2, 2, 1, 0, 1, 0, 1, 2, 0, 1, 1, 2, 1, 0, 1, 0, 2, 1, 0,\n",
        "    2, 1, 0, 1, 0, 2, 1, 0, 2, 1\n",
        "]\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "print(f'tokenizer.word_index: ${tokenizer.word_index}')\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "print(f'sequences: ${sequences}')\n",
        "max_sequence_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "print(f'padded_sequences: ${padded_sequences}')\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "labels = np.eye(3)[labels]\n",
        "print(f'labels: #{labels}')\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build LSTM model\n",
        "embedding_dim = 50\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(LSTM(units=100))\n",
        "model.add(Dense(units=3, activation='softmax')) # 3 output units for 3 classes\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train LSTM model\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Function to predict tone of a text\n",
        "def predict_tone(text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length)\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    tone = np.argmax(prediction)\n",
        "    if tone == 0:\n",
        "        return \"Negative\"\n",
        "    elif tone == 1:\n",
        "        return \"Positive\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "# Example usage\n",
        "test_text = \"This is amazing!\"\n",
        "print(\"Predicted tone:\", predict_tone(test_text))\n",
        "\n"
      ]
    }
  ]
}